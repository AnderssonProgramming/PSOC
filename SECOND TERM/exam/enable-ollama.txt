# ğŸš€ Iniciar Ollama

## Comando para activar el servicio Ollama

```bash
ollama serve
```

## Alternativa con ruta completa (si no estÃ¡ en PATH)

```bash
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" serve
```

## Verificar que Ollama estÃ¡ corriendo

```bash
# Ver modelos instalados
ollama list

# Probar el modelo Phi-3
ollama run phi3 "Hola, Â¿cÃ³mo estÃ¡s?"
```

## âœ… El servicio estÃ¡ listo cuando veas:

```
Listening on 127.0.0.1:11434
```

## ğŸ“ Notas importantes

- **Ollama se ejecuta automÃ¡ticamente en segundo plano** despuÃ©s de la instalaciÃ³n en Windows
- Solo necesitas ejecutar `ollama serve` manualmente si el servicio se detuvo
- El modelo **Phi-3** (2.2 GB) ya estÃ¡ descargado y listo para usar
- La API escucha en: `http://127.0.0.1:11434`

## ğŸ” Verificar si Ollama ya estÃ¡ corriendo

```powershell
# En PowerShell, verifica la conexiÃ³n
Invoke-RestMethod -Uri "http://127.0.0.1:11434/api/tags" | Select-Object -ExpandProperty models
```

Si obtienes una lista con `phi3:latest`, Â¡Ollama ya estÃ¡ activo! âœ…
