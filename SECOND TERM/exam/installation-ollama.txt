# ü§ñ Gu√≠a de Configuraci√≥n del Chatbot con Ollama

## ‚úÖ ¬øPor qu√© Ollama?

- ‚úÖ **100% GRATIS** - Sin l√≠mites de uso
- ‚úÖ **100% LOCAL** - Tus datos nunca salen de tu PC
- ‚úÖ **R√ÅPIDO** - 1-2 segundos por respuesta
- ‚úÖ **F√ÅCIL** - Instalaci√≥n en 2 minutos
- ‚úÖ **POTENTE** - Modelos de √∫ltima generaci√≥n

---

## üì• PASO 1: Instalar Ollama

### Opci√≥n A: Con WinGet (Recomendado)
```powershell
winget install Ollama.Ollama
```

### Opci√≥n B: Descarga manual
1. Ve a https://ollama.ai/download
2. Descarga el instalador para Windows
3. Ejecuta el instalador
4. Ollama se iniciar√° autom√°ticamente en segundo plano

---

## ü§ñ PASO 2: Descargar un modelo

Elige UNO de estos modelos (recomendados de menor a mayor tama√±o):

### üèÉ Phi-3 (RECOMENDADO - M√°s r√°pido)
```powershell
ollama pull phi3
```
- **Tama√±o**: 2.3 GB
- **Velocidad**: ‚ö°‚ö°‚ö°‚ö°‚ö° (1-2 segundos)
- **Calidad**: ‚≠ê‚≠ê‚≠ê‚≠ê (Muy bueno para espa√±ol)
- **Memoria**: 4 GB RAM m√≠nimo

### ‚öñÔ∏è Llama 3.2 (Balance perfecto)
```powershell
ollama pull llama3.2
```
- **Tama√±o**: 3.2 GB
- **Velocidad**: ‚ö°‚ö°‚ö°‚ö° (2-3 segundos)
- **Calidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excelente)
- **Memoria**: 6 GB RAM m√≠nimo

### üéØ Mistral (M√°s potente)
```powershell
ollama pull mistral
```
- **Tama√±o**: 4.1 GB
- **Velocidad**: ‚ö°‚ö°‚ö° (3-4 segundos)
- **Calidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excelente)
- **Memoria**: 8 GB RAM m√≠nimo

**Mi recomendaci√≥n**: Empieza con **phi3** (m√°s r√°pido y ligero).

---

## ‚öôÔ∏è PASO 3: Configurar el backend

Ya est√° configurado autom√°ticamente en tu `.env`:

```env
# Ollama Local (RECOMENDADO - 100% gratis, sin l√≠mites)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3

# OpenAI API Compatible (para usar modelos locales con API OpenAI)
USE_LOCAL_MODEL=true
LOCAL_API_URL=http://localhost:11434/v1
```

**Si descargaste otro modelo**, cambia `OLLAMA_MODEL`:
- Para Llama 3.2: `OLLAMA_MODEL=llama3.2`
- Para Mistral: `OLLAMA_MODEL=mistral`

---

## üöÄ PASO 4: Reiniciar el backend

```powershell
# Si el backend est√° corriendo, se reiniciar√° autom√°ticamente
# Si no, ejecuta:
cd back
npm run start:dev
```

Deber√≠as ver este mensaje en la consola:
```
ü§ñ Usando modelo local Ollama: phi3
```

---

## üß™ PASO 5: Probar el chatbot

1. Ve a **http://localhost:3001/chat**
2. Haz una pregunta:
   ```
   ¬øQu√© es Gender Quest?
   ```
3. Deber√≠a responder en 1-3 segundos con IA local

---

## üîß Soluci√≥n de problemas

### ‚ùå Error: "Ollama no disponible"

**Problema**: Ollama no est√° corriendo.

**Soluci√≥n**:
```powershell
# Verifica si Ollama est√° corriendo
ollama list

# Si no responde, inicia Ollama manualmente
ollama serve
```

En Windows, Ollama normalmente se inicia autom√°ticamente como servicio.

---

### ‚ùå Error: "Model not found"

**Problema**: El modelo no est√° descargado.

**Soluci√≥n**:
```powershell
# Ver modelos instalados
ollama list

# Descargar el modelo
ollama pull phi3
```

---

### ‚ùå La primera respuesta tarda mucho

**Es normal**: El modelo se carga en memoria la primera vez (5-10 segundos).
Las siguientes respuestas ser√°n instant√°neas (1-2 segundos).

---

### ‚ùå Error de memoria

**Problema**: Tu PC no tiene suficiente RAM.

**Soluci√≥n**: Usa un modelo m√°s peque√±o:
```powershell
# Modelo m√°s ligero (1.7 GB)
ollama pull gemma:2b
```

Luego en `.env`:
```env
OLLAMA_MODEL=gemma:2b
```

---

## üìä Comparaci√≥n de Opciones

| M√©todo | Velocidad | Costo | L√≠mites | Privacidad |
|--------|-----------|-------|---------|------------|
| **Ollama (Local)** | ‚ö°‚ö°‚ö°‚ö° | $0 | ‚àû | üîí 100% |
| Hugging Face API | ‚ö°‚ö°‚ö° | $0 | 1000/d√≠a | ‚òÅÔ∏è Cloud |
| Modo Demo | ‚ö°‚ö°‚ö°‚ö°‚ö° | $0 | ‚àû | üîí 100% |

---

## üéØ Comandos √∫tiles de Ollama

```powershell
# Ver modelos instalados
ollama list

# Descargar un modelo
ollama pull <nombre-modelo>

# Eliminar un modelo
ollama rm <nombre-modelo>

# Probar un modelo en la terminal
ollama run phi3

# Ver informaci√≥n del modelo
ollama show phi3

# Actualizar Ollama
winget upgrade Ollama.Ollama
```

---

## üîÑ Cambiar entre m√©todos

### Para usar Ollama (local):
```env
USE_LOCAL_MODEL=true
```

### Para usar Hugging Face (cloud):
```env
USE_LOCAL_MODEL=false
```

El backend autom√°ticamente:
1. Intenta Ollama si `USE_LOCAL_MODEL=true`
2. Si falla, intenta Hugging Face
3. Si falla, usa modo demo

**¬°Siempre funcionar√°!** üéâ

---

## ‚ö° Optimizaci√≥n

### Para respuestas m√°s r√°pidas:
```env
# En .env, puedes ajustar:
OLLAMA_MODEL=phi3  # Modelo m√°s r√°pido
```

### Para respuestas de mejor calidad:
```env
OLLAMA_MODEL=llama3.2  # Mejor calidad
```

---

## üí° Ventajas de Ollama para Gender Quest

‚úÖ **Sin l√≠mites de uso** - Los usuarios pueden hacer 1000+ preguntas sin restricciones
‚úÖ **Privacidad total** - Las conversaciones nunca salen de tu servidor
‚úÖ **Siempre disponible** - No depende de servicios externos
‚úÖ **Gratis para siempre** - $0 de costos operativos
‚úÖ **Escalable** - Puedes mover el servidor a producci√≥n f√°cilmente

---

## üåê Para Producci√≥n

Si quieres desplegar tu app en un servidor:

1. Instala Ollama en el servidor
2. Descarga el modelo que quieras usar
3. Configura el `.env` con la URL del servidor:
   ```env
   OLLAMA_BASE_URL=http://tu-servidor:11434
   ```

¬°Y listo! Tu chatbot funcionar√° 24/7 sin costos.

---

## üìö Recursos adicionales

- Documentaci√≥n oficial: https://ollama.ai/
- Lista de modelos: https://ollama.ai/library
- GitHub de Ollama: https://github.com/ollama/ollama
- Discord de Ollama: https://discord.gg/ollama

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Ollama instalado (`ollama --version`)
- [ ] Modelo descargado (`ollama list`)
- [ ] `.env` configurado con `USE_LOCAL_MODEL=true`
- [ ] Backend reiniciado (ver mensaje "ü§ñ Usando modelo local")
- [ ] Chatbot responde en http://localhost:3001/chat

---

¬°Listo! Ahora tienes un chatbot con IA **100% gratis y sin l√≠mites**. üéâ
